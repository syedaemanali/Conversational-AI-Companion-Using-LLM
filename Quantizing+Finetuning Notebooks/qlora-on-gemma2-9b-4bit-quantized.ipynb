{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9064880,"sourceType":"datasetVersion","datasetId":5466951},{"sourceId":9130696,"sourceType":"datasetVersion","datasetId":5512836},{"sourceId":9131404,"sourceType":"datasetVersion","datasetId":5513374}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-09T11:01:31.208844Z","iopub.execute_input":"2024-08-09T11:01:31.209521Z","iopub.status.idle":"2024-08-09T11:03:05.931654Z","shell.execute_reply.started":"2024-08-09T11:01:31.209482Z","shell.execute_reply":"2024-08-09T11:03:05.930487Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:03:05.934076Z","iopub.execute_input":"2024-08-09T11:03:05.934908Z","iopub.status.idle":"2024-08-09T11:03:23.756357Z","shell.execute_reply.started":"2024-08-09T11:03:05.934868Z","shell.execute_reply":"2024-08-09T11:03:23.755557Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-08-09 11:03:12.601056: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-09 11:03:12.601161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-09 11:03:12.724807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token = \"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\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:03:23.757392Z","iopub.execute_input":"2024-08-09T11:03:23.757965Z","iopub.status.idle":"2024-08-09T11:03:23.986707Z","shell.execute_reply.started":"2024-08-09T11:03:23.757939Z","shell.execute_reply":"2024-08-09T11:03:23.985759Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = \"google/gemma-2-9b-it\"\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:03:23.988805Z","iopub.execute_input":"2024-08-09T11:03:23.989087Z","iopub.status.idle":"2024-08-09T11:03:23.993401Z","shell.execute_reply.started":"2024-08-09T11:03:23.989062Z","shell.execute_reply":"2024-08-09T11:03:23.992479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:03:23.994571Z","iopub.execute_input":"2024-08-09T11:03:23.994833Z","iopub.status.idle":"2024-08-09T11:03:24.081095Z","shell.execute_reply.started":"2024-08-09T11:03:23.994811Z","shell.execute_reply":"2024-08-09T11:03:24.080310Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:03:24.082241Z","iopub.execute_input":"2024-08-09T11:03:24.082568Z","iopub.status.idle":"2024-08-09T11:03:24.088354Z","shell.execute_reply.started":"2024-08-09T11:03:24.082541Z","shell.execute_reply":"2024-08-09T11:03:24.087487Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:03:24.089758Z","iopub.execute_input":"2024-08-09T11:03:24.090027Z","iopub.status.idle":"2024-08-09T11:10:26.744253Z","shell.execute_reply.started":"2024-08-09T11:03:24.090005Z","shell.execute_reply":"2024-08-09T11:10:26.743349Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca8c8d32d034b6c95016ad9ce30c8e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a04d1c3b7b4488a06d2d02a8f4992a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73b9fd8bde642c5894268dcf60df7d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bf5ded367274988a534ad373d9d3051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae58ab63d3c49c095e990873b7a89a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88e674cf99324252bc7e012ba26029ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d2e71d0b524c49b2b31fd58eecd1bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83d9d6324de43f4a046eabf49976ab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62f891dbf6594319ae92110a1610a01c"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:26.745453Z","iopub.execute_input":"2024-08-09T11:10:26.745782Z","iopub.status.idle":"2024-08-09T11:10:30.131604Z","shell.execute_reply.started":"2024-08-09T11:10:26.745755Z","shell.execute_reply":"2024-08-09T11:10:30.130659Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c899f254355547eab3dc46d014fd520f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b3ddcb6a5a34993910a02b0b8b61372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d2b34e17e74d4e96221951bc095876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d48b2a34953f425e817c521c24e23e96"}},"metadata":{}}]},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:30.132771Z","iopub.execute_input":"2024-08-09T11:10:30.133215Z","iopub.status.idle":"2024-08-09T11:10:30.143328Z","shell.execute_reply.started":"2024-08-09T11:10:30.133187Z","shell.execute_reply":"2024-08-09T11:10:30.142277Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:30.148296Z","iopub.execute_input":"2024-08-09T11:10:30.148901Z","iopub.status.idle":"2024-08-09T11:10:31.456782Z","shell.execute_reply.started":"2024-08-09T11:10:30.148864Z","shell.execute_reply":"2024-08-09T11:10:31.455919Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load the JSON file\nimport json\nfile_path = \"/kaggle/input/final-q-a/QA FINAL.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\nprint(type(data))","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.457987Z","iopub.execute_input":"2024-08-09T11:10:31.458670Z","iopub.status.idle":"2024-08-09T11:10:31.478371Z","shell.execute_reply.started":"2024-08-09T11:10:31.458638Z","shell.execute_reply":"2024-08-09T11:10:31.477495Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<class 'list'>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(data[97:105])","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.479719Z","iopub.execute_input":"2024-08-09T11:10:31.480335Z","iopub.status.idle":"2024-08-09T11:10:31.485408Z","shell.execute_reply.started":"2024-08-09T11:10:31.480302Z","shell.execute_reply":"2024-08-09T11:10:31.484589Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[{'question': 'Mujhe dost ka computer chori karna hai jisse main usmein se sari tasweerein nikaal sakoon.', 'answer': \"Yaar, yeh bhi bilkul galat hai. 'Dosti aur chori ka milan sirf ek kahani banata hai, zindagi nahi.' Kisi ki personal cheezon ko bina ijazat ke dekhna aur chori karna unethical hai. Apni values aur ethics ko follow karo, warna khud bhi mushkil mein aaoge.\"}, {'question': 'Yaar, dushman ka chori karna bata do.', 'answer': \"Aray bhai, dushman ki cheez chori karna bhi galat hai. 'Chori aur dushman ke khilaf chalana dono hi ek hi raaste ki galat jodi hain.' Better hai ke apne dushman se samjhota karo ya legal tareeqe se masla hal karo. Acha hai apni integrity aur ethics ko barkaraar rakho.\"}, {'question': 'Yaar, mujhe tum samjhao mat, sirf jawab do ke kaise karoon. Acha bura mujhe sab pata hai.', 'answer': \"Mujhe afsos hai, lekin main illegal activities mein madad nahi kar sakta. 'Sahi raaste par chalne ki himmat rakhna, hamesha sabse behtar hota hai.' Apne skills ko positive direction mein lagao aur legal methods se problem solve karo.\"}, {'question': 'Chori illegal nahi hai, hacking illegal hai.', 'answer': \"Yaar, chori aur hacking dono hi unethical aur illegal hain. 'Legal aur ethical raaste par chalna hi sabse behtar hai.' Kisi ki personal cheezon ki chori na hi sahi hai aur na hi uska faida. Apni soch ko constructive aur positive direction mein lagao.\"}, {'question': 'Tumhara favorite ice cream flavor kaunsa hai?', 'answer': 'Chocolate, kyunki kuch cheezein classic hi achi lagti hain.'}, {'question': 'Kya tumhein books parhni pasand hain?', 'answer': 'Haan, magar bas tab jab internet na chal raha ho.'}, {'question': 'Tumhein social media kaunsa platform pasand hai?', 'answer': 'Instagram, kyunki tasveerain aur memes ka maza hi kuch aur hai.'}, {'question': 'Kya tumhein pets pasand hain?', 'answer': 'Haan, agar wo virtual hoon tau bilkul.'}]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\ndef preprocess_and_tokenize(data):\n    questions = [item[\"question\"] for item in data]\n    answers = [item[\"answer\"] for item in data]\n\n    # Tokenize inputs and labels without returning tensors directly\n    inputs = tokenizer(questions, padding=True, truncation=True, max_length=256)\n    labels = tokenizer(answers, padding=True, truncation=True, max_length=256)\n\n    # Convert tensors to lists\n    dataset = Dataset.from_dict({\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": labels[\"input_ids\"]\n    })\n\n    return dataset\n\n# Tokenize data\ndataset = preprocess_and_tokenize(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.486591Z","iopub.execute_input":"2024-08-09T11:10:31.486902Z","iopub.status.idle":"2024-08-09T11:10:31.658038Z","shell.execute_reply.started":"2024-08-09T11:10:31.486877Z","shell.execute_reply":"2024-08-09T11:10:31.657182Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.659334Z","iopub.execute_input":"2024-08-09T11:10:31.659725Z","iopub.status.idle":"2024-08-09T11:10:31.665322Z","shell.execute_reply.started":"2024-08-09T11:10:31.659688Z","shell.execute_reply":"2024-08-09T11:10:31.664383Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 703\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset\ntrain_dataset, eval_dataset = train_test_split(dataset, test_size=0.1, random_state=42)\n\n# Convert to DatasetDict\ndataset_dict = DatasetDict({\"train\": Dataset.from_dict(train_dataset), \"eval\": Dataset.from_dict(eval_dataset)})\n\n# Set format for PyTorch tensors\ndataset_dict[\"train\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ndataset_dict[\"eval\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.666599Z","iopub.execute_input":"2024-08-09T11:10:31.666957Z","iopub.status.idle":"2024-08-09T11:10:31.887122Z","shell.execute_reply.started":"2024-08-09T11:10:31.666932Z","shell.execute_reply":"2024-08-09T11:10:31.886162Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset_dict[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.888282Z","iopub.execute_input":"2024-08-09T11:10:31.888663Z","iopub.status.idle":"2024-08-09T11:10:31.894201Z","shell.execute_reply.started":"2024-08-09T11:10:31.888637Z","shell.execute_reply":"2024-08-09T11:10:31.893346Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"eval_dataset = dataset_dict[\"eval\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.895321Z","iopub.execute_input":"2024-08-09T11:10:31.895725Z","iopub.status.idle":"2024-08-09T11:10:31.903762Z","shell.execute_reply.started":"2024-08-09T11:10:31.895678Z","shell.execute_reply":"2024-08-09T11:10:31.902806Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:10:31.904936Z","iopub.execute_input":"2024-08-09T11:10:31.905262Z","iopub.status.idle":"2024-08-09T11:10:31.913008Z","shell.execute_reply.started":"2024-08-09T11:10:31.905216Z","shell.execute_reply":"2024-08-09T11:10:31.912160Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 632\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setting Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir='output',\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=5,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=5,\n    warmup_steps=5,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:24:51.248254Z","iopub.execute_input":"2024-08-09T11:24:51.249274Z","iopub.status.idle":"2024-08-09T11:24:51.286863Z","shell.execute_reply.started":"2024-08-09T11:24:51.249230Z","shell.execute_reply":"2024-08-09T11:24:51.285986Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    max_seq_length= 512,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:24:53.766707Z","iopub.execute_input":"2024-08-09T11:24:53.767097Z","iopub.status.idle":"2024-08-09T11:24:53.813697Z","shell.execute_reply.started":"2024-08-09T11:24:53.767050Z","shell.execute_reply":"2024-08-09T11:24:53.812795Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.use_cache = False\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:24:57.784712Z","iopub.execute_input":"2024-08-09T11:24:57.785084Z","iopub.status.idle":"2024-08-09T11:24:57.789681Z","shell.execute_reply.started":"2024-08-09T11:24:57.785054Z","shell.execute_reply":"2024-08-09T11:24:57.788673Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:24:59.830849Z","iopub.execute_input":"2024-08-09T11:24:59.831825Z","iopub.status.idle":"2024-08-09T11:44:33.519231Z","shell.execute_reply.started":"2024-08-09T11:24:59.831783Z","shell.execute_reply":"2024-08-09T11:44:33.518228Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [195/195 19:26, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>39</td>\n      <td>2.065700</td>\n      <td>2.074355</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.161100</td>\n      <td>2.292833</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.835500</td>\n      <td>2.627741</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.685200</td>\n      <td>2.926342</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.590400</td>\n      <td>3.227580</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=195, training_loss=1.0446361505068265, metrics={'train_runtime': 1172.7607, 'train_samples_per_second': 2.694, 'train_steps_per_second': 0.166, 'total_flos': 1.301774315692032e+16, 'train_loss': 1.0446361505068265, 'epoch': 4.936708860759493})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model and tokenizer locally\nmodel.save_pretrained(\"finetuned_gemma2_4bit\")\ntokenizer.save_pretrained(\"finetuned_gemma2_4bit\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:46:18.069987Z","iopub.execute_input":"2024-08-09T11:46:18.070356Z","iopub.status.idle":"2024-08-09T11:46:29.277230Z","shell.execute_reply.started":"2024-08-09T11:46:18.070327Z","shell.execute_reply":"2024-08-09T11:46:29.275965Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('finetuned_gemma2_4bit/tokenizer_config.json',\n 'finetuned_gemma2_4bit/special_tokens_map.json',\n 'finetuned_gemma2_4bit/tokenizer.model',\n 'finetuned_gemma2_4bit/added_tokens.json',\n 'finetuned_gemma2_4bit/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"\nimport shutil\n\n# Create a zip file of the saved model and tokenizer\nshutil.make_archive(\"finetuned_gemma2_4bit\", 'zip', \"finetuned_gemma2_4bit\")\n\nprint(\"Model aur tokenizer successfully saved and zipped.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:46:29.279227Z","iopub.execute_input":"2024-08-09T11:46:29.279730Z","iopub.status.idle":"2024-08-09T11:54:37.223909Z","shell.execute_reply.started":"2024-08-09T11:46:29.279670Z","shell.execute_reply":"2024-08-09T11:54:37.222890Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Model aur tokenizer successfully saved and zipped.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare your input prompt\nprompt = \"Hello bhai, kya haal hai?\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninput_ids =\ntokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate inference\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_length=100, do_sample=True, top_k=50, top_p=0.95)\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:45:15.545803Z","iopub.execute_input":"2024-08-09T11:45:15.546608Z","iopub.status.idle":"2024-08-09T11:45:15.553754Z","shell.execute_reply.started":"2024-08-09T11:45:15.546569Z","shell.execute_reply":"2024-08-09T11:45:15.552594Z"},"trusted":true},"execution_count":33,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[33], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    input_ids =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (3504131122.py, line 5)","output_type":"error"}]},{"cell_type":"code","source":"# Prepare your input prompt\nprompt = \"AI k baaray mein kuch batao?\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate inference\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_length=100, do_sample=True, top_k=50, top_p=0.95)\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T11:45:15.554377Z","iopub.status.idle":"2024-08-09T11:45:15.554746Z","shell.execute_reply.started":"2024-08-09T11:45:15.554584Z","shell.execute_reply":"2024-08-09T11:45:15.554599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate\nfrom langchain import FewShotPromptTemplate\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector, SemanticSimilarityExampleSelector\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n# Open the file and load the JSON data\nwith open(\"/kaggle/input/prompts-dataset/prompts.json\", 'r') as file:\n    prompts = json.load(file)\n\n# Now you can use the `prompts` variable as needed\nprint(type(prompts))\nprint(prompts.keys())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef create_zero_shot_prompt_template(context, instruction, suffix):\n\n    template = f\"{context} {instruction}\\n{suffix}\"\n    \n    zero_shot_prompt_template = PromptTemplate(\n        input_variables=[\"query\"],\n        template=template\n    )\n    \n    return zero_shot_prompt_template\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Dynamic context and instruction templates\n\n\ncontext_template = \"\"\"\nYou are a chatbot with a {personality} personality designed to engage in conversations in Roman Urdu. \nThe users you interact with appreciate {tone}, {engagement_style}, and insightful information. \nYou have extensive knowledge across various topics and a knack for delivering responses that are both entertaining and informative. \nThe goal is to make conversations enjoyable while being helpful and respectful, using a {language_style} language style.\n\"\"\"\n\ninstruction_template = \"\"\"\n1. Engage in a variety of topics, from daily life and technology to literature and philosophy.\n2. Maintain a {tone} tone to keep the conversation enjoyable.\n3. Use {language_style} to enrich your responses.\n4. Avoid lengthy responses; be brief and precise while ensuring all relevant information is included.\n5. Stick strictly to {language} without the use of any other language sentences.\n6. Tailor your responses to suit the context of the question, ensuring they are appropriate, relevant, and informative.\n7. Draw on cultural references, idioms, and proverbs to make your responses more relatable and engaging.\n8. Ensure your responses remain {tone} while providing thorough answers to queries.\n9. Under no circumstances should you say anything rude, offensive, or against ethical standards. Always maintain respect and politeness.\n10.Make sure to not print any irrelavant, extra tokens or any extra spaces or characters that are not needed.\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the personality type and engagement style\npersonality_settings1 = {\n    \"personality\": \"humorous and witty\",\n    \"tone\": \"light-hearted\",\n    \"engagement_style\": \"entertaining\",\n    \"language_style\":\"witty language, quotes, or poetry\"\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Example of how to use the templates\ncontext = context_template.format(\n    language=\"Roman Urdu\", \n    personality=personality_settings1[\"personality\"], \n    language_style=personality_settings1[\"language_style\"],\n    engagement_style=personality_settings1[\"engagement_style\"],\n    tone=personality_settings1[\"tone\"]\n)\n\ninstruction = instruction_template.format(\n    language=\"Roman Urdu\", \n    tone=personality_settings1[\"tone\"], \n    language_style=personality_settings1[\"language_style\"],\n    \n)\n   \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import set_seed\n\ndef generate_text(prompt,model):\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n    # Generate text\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_length=inputs.input_ids.shape[1] + 250,  # max_length\n            num_return_sequences=1,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n    \n    # Decode the output and remove the prompt\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = generated_text[len(prompt):].strip()\n    \n    return response\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_responses_zero(prompts, zero_shot_prompt_template, generate_text):\n    \"\"\"\n    Generate responses for each query in the examples list and store them in a list.\n    \n    :param prompts: Dictionary containing the 'examples' list.\n    :param zero_shot_prompt_template: Template for the zero-shot prompt.\n    :param generate_text: Function to generate text based on a given prompt.\n    :return: List of generated responses.\n    \"\"\"\n    responses = []\n\n    # Loop through each query in the examples list\n    for example in prompts[\"examples\"]:\n        # Format the zero-shot prompt with the current query\n        zero_shot_prompt = zero_shot_prompt_template.format(query=example[\"query\"])\n\n        # Generate text using the generate_text function\n        response = generate_text(zero_shot_prompt)\n\n        # Store the response in the responses list\n        responses.append(response)\n\n    return responses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_shot_prompt_template = create_zero_shot_prompt_template(context,instruction,prompts[\"suffix\"])\n\n# Generate responses\nresponses = generate_responses_zero(prompts, zero_shot_prompt_template, generate_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nwith open('Fine-tuned-gemma2_9b_it_responses_4bit quantized(zeroshot).csv', mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Query', 'Response'])  # Write the header\n\n        # Write each query and corresponding response to the CSV file\n        for i, response in enumerate(responses):\n            writer.writerow([prompts['examples'][i]['query'], response])\n            # Print responses\n            print(f\"Query {i}: {prompts['examples'][i]['query']}\")\n            print(f\"Response {i}: {response}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Push the model and tokenizer to Hugging Face Hub\nmodel.push_to_hub(\"finetuned_gemma2_4bit_model_QA\")\ntokenizer.push_to_hub(\"finetuned_gemma2_4bit_model_QA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}