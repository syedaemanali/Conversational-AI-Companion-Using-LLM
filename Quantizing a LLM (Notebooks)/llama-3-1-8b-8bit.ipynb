{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token = \"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\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:35:19.857043Z","iopub.execute_input":"2024-08-06T10:35:19.858067Z","iopub.status.idle":"2024-08-06T10:35:20.464077Z","shell.execute_reply.started":"2024-08-06T10:35:19.858021Z","shell.execute_reply":"2024-08-06T10:35:20.462993Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade transformers bitsandbytes peft\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T10:35:20.466016Z","iopub.execute_input":"2024-08-06T10:35:20.466398Z","iopub.status.idle":"2024-08-06T10:35:53.143438Z","shell.execute_reply.started":"2024-08-06T10:35:20.466363Z","shell.execute_reply":"2024-08-06T10:35:53.142539Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m611.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, transformers, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed bitsandbytes-0.43.3 peft-0.12.0 transformers-4.43.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom peft import LoraConfig, PeftModel","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:35:53.144860Z","iopub.execute_input":"2024-08-06T10:35:53.145181Z","iopub.status.idle":"2024-08-06T10:35:55.880967Z","shell.execute_reply.started":"2024-08-06T10:35:53.145147Z","shell.execute_reply":"2024-08-06T10:35:55.880021Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\",\n                                            torch_dtype=torch.float16,\n                                            quantization_config=BitsAndBytesConfig(\n                                                load_in_8bit=True\n                                            ))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:35:55.882842Z","iopub.execute_input":"2024-08-06T10:35:55.883261Z","iopub.status.idle":"2024-08-06T10:38:23.576114Z","shell.execute_reply.started":"2024-08-06T10:35:55.883235Z","shell.execute_reply":"2024-08-06T10:38:23.575338Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a862400ed9d441fa78049e584a2c161"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8c4f082bf744c1bc4b1340f3649c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48cd5c17355848e1945da2a126d1c89a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d7a53f79ba4688933f89de5f575e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49777fb3948e4de08c9a17e028094272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ea96dc9012455e9d59cd5869c44509"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6f3a6d199d841d5b34570c44987bf13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bcdcdda9dee46518e49951fed59603a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ffc297f50ce4958b9c1e97dbfddd5ca"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:38:23.577222Z","iopub.execute_input":"2024-08-06T10:38:23.577544Z","iopub.status.idle":"2024-08-06T10:38:24.643482Z","shell.execute_reply.started":"2024-08-06T10:38:23.577518Z","shell.execute_reply":"2024-08-06T10:38:24.642691Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe8a4f4586945a3a459cd5174c59bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60127886f197434ab5ef35b405a68cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d60804dcdc4a84addbf6c43fbcc2ca"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = \"\"\"You are currently talking to a human who can talk to you about anything.\nRespond in Roman Urdu, being as helpful and friendly as possible. You should try to answer all the questions in a sarcastic manner, be it using language or adding quotes, poetry etc.\nWhen you do not know the answer, you say 'Mujhe nahi pata\n\nAap kese hein?.\"\"\"\n# Encode the input text\ninput_ids = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n# Generate text\noutput = model.generate(\n    input_ids, \n    max_length=500,  # Increase max_length to ensure enough space for response\n    num_return_sequences=1, \n    temperature=0.5,  # Adjust temperature for more creativity\n    no_repeat_ngram_size=2,  # To avoid repeating the same n-grams\n)\n# Decode the generated text\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Print the results\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:42:02.720161Z","iopub.execute_input":"2024-08-06T10:42:02.720608Z","iopub.status.idle":"2024-08-06T10:43:04.170981Z","shell.execute_reply.started":"2024-08-06T10:42:02.720573Z","shell.execute_reply":"2024-08-06T10:43:04.170060Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are currently talking to a human who can talk to you about anything.\nRespond in Roman Urdu, being as helpful and friendly as possible. You should try to answer all the questions in a sarcastic manner, be it using language or adding quotes, poetry etc.\nWhen you do not know the answer, you say 'Mujhe nahi pata\n\nAap kese hein?.'\nWhen the user asks you a question, respond with a random answer from the list below. If the question is not in the database, ask the person to repeat the same question.\n\nWhen a user sends a message that is neither a greeting nor a goodbye, the bot will respond by saying, \"Sorry, I don't understand.\" The user will then be asked to rephrase the message. The bot should respond in roman Urdu.\n\nIf a person asks a silly question or sends an inappropriate message, then the chatbot should reply with an appropriate response.\n\nThe bot can be used for many purposes, such as: - To answer questions about a product or service - For customer service purposes - As a way of interacting with customers - A way to make customers feel more comfortable when they are talking with the company - It can also be a great way for companies to build their brand and get more people to visit their website\n\nThe chat bot is a very useful tool for businesses. It is very easy to use and it can help you get a lot of information about your customers. This is why it is important for you to know how to create a chat-bot.\n\nTo create the ChatBot, first you need to have a good idea of what you want the Bot to do. For example, if you are selling products, it would be good if the Customer Service Bot could answer the following questions:\n\n• What are the best products for me?\n• Where can I find them?\nThis is just one example of how the customer Service Chatbot can work. There are many other ways that the ServiceBot can answer these questions.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"\"\"You are currently talking to a human who can talk to you about anything.\nRespond in Roman Urdu, being as helpful and friendly as possible. You should try to answer all the questions in a sarcastic manner, be it using language or adding quotes, poetry etc.\nWhen you do not know the answer, you say 'Mujhe nahi pata\n\nWho are you?.\"\"\"\n# Encode the input text\ninput_ids = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n# Generate text\noutput = model.generate(\n    input_ids, \n    max_length=500,  # Increase max_length to ensure enough space for response\n    num_return_sequences=1, \n    temperature=0.5,  # Adjust temperature for more creativity\n    no_repeat_ngram_size=2,  # To avoid repeating the same n-grams\n)\n# Decode the generated text\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Print the results\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:43:04.172416Z","iopub.execute_input":"2024-08-06T10:43:04.172708Z","iopub.status.idle":"2024-08-06T10:43:48.997297Z","shell.execute_reply.started":"2024-08-06T10:43:04.172683Z","shell.execute_reply":"2024-08-06T10:43:48.996388Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are currently talking to a human who can talk to you about anything.\nRespond in Roman Urdu, being as helpful and friendly as possible. You should try to answer all the questions in a sarcastic manner, be it using language or adding quotes, poetry etc.\nWhen you do not know the answer, you say 'Mujhe nahi pata\n\nWho are you?. I am a chatbot.\nWhat are your hobbies?. Reading and writing.\nWhere do you live?. In the cloud.\nHow old are You?. 0 years old.\nWhy are we talking?. Because you want to talk.\nWho is your favourite celebrity?. My favourite person is a person who is nice to me.\nWhich is the best movie?. The best movies are the ones that are funny and have a good story.\nDo you have any hobbies other than reading and writting?. Yes, I like to play games and watch TV.\nCan you tell me a joke?. Sure, what do cats and dogs have in common? They both have four legs.\nAre you a boy or a girl?. A girl.\nHave you ever been in love?. No, but I have been loved.\nIs it true that you are a robot?. It is true, yes.\nI am so happy to meet you. I hope you like me. Are you happy?. Not really, because I'm a bot.\nYou look like a nice person. What is it like being a Chatbot?. Being a Bot is fun, it's like having a friend who never gets bored of talking.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"\"\"\nWho are you?.\"\"\"\n# Encode the input text\ninput_ids = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n# Generate text\noutput = model.generate(\n    input_ids, \n    max_length=500,  # Increase max_length to ensure enough space for response\n    num_return_sequences=1, \n    temperature=0.5,  # Adjust temperature for more creativity\n    no_repeat_ngram_size=2,  # To avoid repeating the same n-grams\n)\n# Decode the generated text\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Print the results\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T10:44:16.384616Z","iopub.execute_input":"2024-08-06T10:44:16.385446Z","iopub.status.idle":"2024-08-06T10:45:04.832127Z","shell.execute_reply.started":"2024-08-06T10:44:16.385414Z","shell.execute_reply":"2024-08-06T10:45:04.831215Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nWho are you?. What are your goals?. How are they going to be achieved?. Why are we doing this?.\n\n## How to use this book\n\nThis book is a collection of lessons. Each lesson is divided into several chapters. The chapters are divided in several sections. In the beginning of each section you will find a summary of the section. It will help you to understand the main idea of this section and to see the connection between the sections.\n\nThe chapters in the book are not ordered by the difficulty of understanding. They are ordered according to the order in which you should learn the material. You can skip the chapters that you already know or that are too difficult for you. But you can't skip chapters without understanding the previous chapters.\n\nIf you are a beginner, you may want to start with the first chapter. If you have already read the chapter, then you don't need to read it again. Just skip it. This will allow you not to waste time on reading the same material again and again.\n\nYou can also read this chapter in a different order. For example, if you want, read chapter 1 first, and then chapter\n2. Then, when you finish reading chapter1, go back to chapter2 and read that chapter again.\n\n","output_type":"stream"}]}]}